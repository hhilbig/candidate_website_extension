# ── Candidate Website Extension Configuration ──

# Which offices and years to scrape
scope:
  house:
    years: [2018, 2020, 2022, 2024]
  senate:
    years: [2002, 2004, 2006, 2008, 2010, 2012, 2014, 2016, 2018, 2020, 2022, 2024]

# Wayback Machine settings
wayback:
  rate_limit_seconds: 0.1       # Delay between requests (100ms default)
  max_retries: 3                # Retries per request
  backoff_factor: 2             # Exponential backoff multiplier
  backoff_max_seconds: 360      # Max backoff (6 minutes, for 429 responses)
  timeout_connect: 30           # Connection timeout (seconds)
  timeout_read: 120             # Read timeout (seconds)
  user_agent: "CandidateWebsiteExtension/1.0 (Academic Research)"

# Scraping behavior
scraping:
  max_subpage_depth: 1          # How many levels of internal links to follow
  max_subpages: 50              # Max subpages per snapshot
  threads: 8                    # Parallel threads for scraping
  text_separator: "#+#"         # Separator between text chunks
  skip_extensions: [".pdf", ".jpg", ".png", ".gif", ".mp3", ".mp4", ".zip"]
  exclude_domains: ["twitter.com", "facebook.com", "instagram.com", "youtube.com"]

# Ballotpedia settings (for campaign website URL lookup)
ballotpedia:
  rate_limit_seconds: 1.0       # Respectful rate limiting (1 req/sec)
  max_retries: 2
  user_agent: "CandidateWebsiteExtension/1.0 (Academic Research)"

# Candidate roster sources
roster:
  fec_bulk_url: "https://www.fec.gov/files/bulk-downloads/{year}/cn{year}.zip"
  ballotpedia_base: "https://ballotpedia.org"

# Output settings
output:
  base_dir: "data"              # Root output directory
  roster_dir: "data/rosters"    # Candidate roster CSVs
  snapshots_dir: "data/snapshots"  # Scraped snapshot CSVs
  progress_dir: "data/progress" # Checkpoint/progress files
